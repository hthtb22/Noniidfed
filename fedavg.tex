\documentclass{article} 
\usepackage{amsmath,amsthm}     
\usepackage{graphicx}     
\usepackage{hyperref} 
\usepackage{url}
\usepackage{amsfonts} 
\usepackage{multicol}
\usepackage{tikz}
\usepackage{smartdiagram}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage[ruled,vlined]{algorithm2e}
\usetikzlibrary{fit}
\tikzset{
	comp/.style = {
		minimum width  = 8cm,
		minimum height = 4.5cm,
		text width     = 8cm,
		inner sep      = 0pt,
		text           = green,
		align          = center,
		font           = \Huge,
		transform shape,
		thick
	},
	monitor/.style = {draw = none, xscale = 18/16, yscale = 11/9},
	display/.style = {shading = axis, left color = black!60, right color = black},
	ut/.style      = {fill = gray}
}
\tikzset{
	computer/.pic = {
		% screen (with border)
		\node(-m) [comp, pic actions, monitor]
		{\phantom{\parbox{\linewidth}{\tikzpictext}}};
		% display (without border)
		\node[comp, pic actions, display] {\tikzpictext};
		\begin{scope}[x = (-m.east), y = (-m.north)]
			% filling the lower part
			\path[pic actions, draw = none]
			([yshift=2\pgflinewidth]-0.1,-1) -- (-0.1,-1.3) -- (-1,-1.3) --
			(-1,-2.4) -- (1,-2.4) -- (1,-1.3) -- (0.1,-1.3) --
			([yshift=2\pgflinewidth]0.1,-1);
			% filling the border of the lower part
			\path[ut]
			(-1,-2.4) rectangle (1,-1.3)
			(-0.9,-1.4) -- (-0.7,-2.3) -- (0.7,-2.3) -- (0.9,-1.4) -- cycle;
			% drawing the frame of the whole computer
			\path[pic actions, fill = none]
			(-1,1) -- (-1,-1) -- (-0.1,-1) -- (-0.1,-1.3) -- (-1,-1.3) --
			(-1,-2.4) coordinate(sw)coordinate[pos=0.5] (-b west) --
			(1,-2.4) -- (1,-1.3) coordinate[pos=0.5] (-b east) --
			(0.1,-1.3) -- (0.1,-1) -- (1,-1) -- (1,1) -- cycle;
			% node around the whole computer
			\node(-c) [fit = (sw)(-m.north east), inner sep = 0pt] {};
		\end{scope}
	}
}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\E}{\mathbb{E}}
\usetikzlibrary{positioning,chains,fit,shapes,calc}
\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem*{remark}{Remark}
\newtheorem{proposition}{Proposition}

\allowdisplaybreaks
\usepackage{collectbox}
\makeatletter
\newcommand{\mybox}{%
	\collectbox{%
		\setlength{\fboxsep}{1pt}%
		\fbox{\BOXCONTENT}%
	}%
}
\@addtoreset{footnote}{page}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
	\section*{Report}
	
		\begin{tabular}{ | m{7em} | m{6.5cm}|  }  
			\hline
			Project & \textbf{Convergence guarantees for gradient descent methods on optimization for non-convex function approximation} \\
			\hline
			Student & \textbf{Hoang Trung Hieu}\\
			\hline 
			Supervisor & \textbf{Horváth Tomáš} \\
			\hline
			 Program & MSc, Applied Mathematics \\
			\hline			
			Semester & 2020/2021, Autumn \\
			\hline
		\end{tabular}
	\subsection*{Summary}
	\subsection*{Process and Results}
	- 
	\subsection*{Future work}
	\subsection*{}
	\begin{thebibliography}{5}
		
		
		\bibitem{cite1}
		F. Sattler, S. Wiedemann, K. Müller, W. Samek,  (2019), \textit{Robust and communication-efficient federated learning from non-iid data},IEEE transactions on neural networks and learning systems,
		\bibitem{cite2}
		H. Brendan McMahan, Eider Moore, Daniel Ramage, (2017)  \textit{Communication-Efficient Learning of Deep Networks
			from Decentralized Data}, \href{https://arxiv.org/abs/1602.05629}{https://arxiv.org/abs/1602.05629}  
		\bibitem{cite3}
		\textit{TensorFlow Federated},\href{ https://github.com/tensorflow/federated }{https://github.com/tensorflow/federated}
		\bibitem{cite4}
	\textit{Advanced Machine Learning Systems Course} , (2017),  \href{https://www.cs.cornell.edu/courses/cs6787/2017fa/}{https://www.cs.cornell.edu/courses/cs6787/2017fa/}	
		\bibitem{cite4}
		X. Li, K. Huang, W. Yang, S. Wang, Z. Zhang, (2019)  \textit{On the Convergence of FedAvg on Non-IID Data}, \href{https://arxiv.org/abs/1907.02189}{https://arxiv.org/abs/1907.02189}
		
	\end{thebibliography}
	\newpage
	\title{Non-convex optimization in federated learning}
	
	\author{Hoang Trung Hieu
		%\scriptsize \\    
	%Eötvös Loránd University \\               
		%hthtb22@gmail.com
	}                      
	\date{2020}
	\maketitle
	
	\noindent
	\begin{abstract}
		abstract-text
	\end{abstract}
	\section{Introduction}
	\subsection{Federated Learning}
	A new approach to train the model without access to the user's data is embedded machine learning. It can be described by two criteria: data never leaves the local devices and updates are communicated instead model. There are a few underlying distributed structure: peer-to-peer learning, distributed training, on-device inference and federated learning (\cite{cite1}). In this project, we consider the new paradigm in Machine Learning-Federated Learning which is a machine learning setting where multiple entities collaborate in solving a learning problem, without directly exchanging data. The Federated training process is coordinated by a central server. It works like this: your device downloads the current model, improves it by learning from data on your phone, and then summarizes the changes as a small focused update. Only this update to the model is sent to the cloud, using encrypted communication, where it is immediately averaged with other user updates to improve the shared model. All the training data remains on your device, and no individual updates are stored in the cloud.\\
	On the other hand, we face up to new challenges: \begin{itemize}
		\item \textit{Communication}.  Total communication can be calculated by the multiplication of the number of communication rounds by the number of parameters by average codeword length.It can be up to hundreds terabyte excluding the upload and download progress, but we do not mention it in this project. 
		\item \textit{Massively distributed}. The number of mobile device owners is massively bigger than average of the number of training samples on each device.
		\item \textit{Unbalanced}. Some users produce significantly more data than others.
		\item \textit{Non-IID}.	The data generated by each user are quite different.
	
\begin{center}
	\begin{tikzpicture}
		\pic(comp0) [
		draw,
		fill = gray!50,
		scale = 0.25,
		pic text = {Center server}
		]
		{computer};
		\path(comp0-c.center) pic 
		foreach[count=\i] \farbe in {yellow, orange, red, red!50!blue, blue, green, black,white}
		(comp\i) [
		draw = \farbe,
		fill = \farbe!30,
		display/.append style = {left color=\farbe!80!black!80},
		scale = 0.1,
		pic text = {Device \i \\Gradient updates}		
		] at +(45*\i:3){computer};	
		\foreach \i in {1,2,3,4,5,6,7,8} \draw (comp\i-c) -- (comp0-c)  node [midway, above, sloped] (TextNode) {\tiny{model}};
		
	\end{tikzpicture}
\begin{center}
	\figurename[1]{. Federated Leaning structure}
\end{center}
\end{center}		
	\end{itemize}
	\subsection{Problem description}
	Let us consider the following distributed optimization model in which $N$ clients cooperatively conquer   $$\min_{w} f(w) = \min_{w} \sum_{k=1}^{N} p_k f_k(w)$$
	where $p_k$ be the weight of $k-$th device and typically it is the proportion of the local data volume in global data volume.i.e $p_k \ge 0, \quad  \sum_{i=1}^{N}p_k=1.$ The $k-$th client has $n_k$ training data which is denoted $x_{k,1} , \cdots,  x_{k,n_k}.$ The local generalization error function are defined by $$f_k(w)= \frac{1}{n_k} \sum_{j=1}^{n_k} \ell (w;x_{k,j})$$
	where $\ell$ is a user loss function of the prediction.\\
	The local generalization also can be defined by taking the expected value of training error of model parameters $w$ over local data $\xi_k \sim \mathcal{D}_k$ $$f_k(w)=\E_{\xi_k \sim \mathcal{D}_k} F(w; \xi_k)$$
	In case of the data is balanced (the volume of each device's data are equal) $$f(x)=\frac{1}{N}  \sum_{i=1}^{N} \tilde{f_k}(w) \quad \text{where} \ \ \tilde{f_k}(w) =p_kNf_k(w)$$
	\section{Algorithms}
	There are two most widely-used FL mechanisms, FedSGD and FedAvg. In general, FedSGD is barely influenced by the none
	independent and identically distributed (non-IID) data problem, but FedAvg suffers from a decline in accuracy. On the other hand, FedAvg is more efficient than FedSGD regarding time consumption and communication.
	\subsection{FedSGD}
Deep learning training mainly relies on variants of stochastic gradient descent, where gradients are computed on a random subset of the total dataset and then used to make one step of the gradient descent.

Federated stochastic gradient descent[13] is the direct transposition of this algorithm to the federated setting, but by using a random fraction C of the nodes and using all the data on this node. The gradients are averaged by the server proportionally to the number of training samples on each node, and used to make a gradient descent step.

	\subsection{FedAvg}
	
	\begin{algorithm}[H]
		\SetAlgoLined
		\textbf{Server executes:} \\
		initialization $w_0$ \;
		\For{each round $t=1,2, \cdots $}{
			 $m \leftarrow \max(C \cdot K,1)$\\
			 $S_t \leftarrow $ (random set of $m$ client) \\
			\For{each client $k \in S_t$} {	
			$w_{t+1}^k \leftarrow \text{ClientUpdate}(k, w_t)$\;			
			}{
				$w_{t+1} \leftarrow \sum_{k=1}^{K} \dfrac{n_k}{n} w_{t+1}^k$\;
			}
		} 
	\textbf{ClientUpdate($k,w$):}\\
	$B \leftarrow \text{ (split} \ P_k	\ \text{into batches of size} \ B )$\\
	\For{each local epoch}{
	\For{batch $b \in B$}{
	$ w \leftarrow w - \eta \nabla \l(w;b)$ }}
		\caption{FederatedAveraging(FedAvg).}
	\end{algorithm}
Federated averaging (FedAvg) is a generalization of FedSGD, which allows local nodes to perform more than one batch update on local data and exchanges the updated weights rather than the gradients. The rationale behind this generalization is that in FedSGD, if all local nodes start from the same initialization, averaging the gradients is strictly equivalent to averaging the weights themselves. Further, averaging tuned weights coming from the same initialization does not necessarily hurt the resulting averaged model's performance.
	\section{Non-Convex optimization}
Machine learning algorithms use optimization all the time. We minimize loss, or error, or maximize some kind of score functions. Gradient descent is the "hello world" optimization algorithm covered on probably any machine learning course. It is obvious in the case of regression, or classification models, but even with tasks such as clustering we are looking for a solution that optimally fits our data (e.g. k-means minimizes the within-cluster sum of squares). So if you want to understand how the machine learning algorithms do work, learning more about optimization helps. Moreover, if you need to do things like hyperparameter tuning, then you are also directly using optimization.

One could argue that convex optimization shouldn't be that interesting for machine learning since instead of dealing with convex functions, we often face up to the composition of convex function which usually is not convex. For examples, matrix completition, principle component analysis, low-rank models and tensor decomposition, maximum likelihood estimation with hidden variables and the most important, deep neural networks.\\
In order to find convergence guarantees for non-convex function(even convex) is quite hard or unrealistic in deep learning. But there are a few assumptions that appears frequently in recent papers.
	\begin{assumption}
		\textbf{$L-$ Lipschitz gradient}: Local objective functions $f_i$ are all $L-$ smooth: $$\left\|\nabla f_i(u)-\nabla f_i(v) \right\| \leq L \left \| u-v \right\|$$
		Or it can be rewritten: $$f_i(u) \le f_i(v)+(u-v)^{T} \nabla f_i(v)+\frac{L}{2} \|u-v\|^2$$
	\end{assumption}
	\begin{assumption}
		\textbf{Bounded variance} $$\E _{\xi_k \sim D_i} \left[\| \nabla \| ^2\right] \le ^2 $$
	\end{assumption}
\begin{assumption}
	\textbf{Bounded gradient} $$\E _{\xi_k \sim D_i} \left[\| \nabla \| ^2\right] \le ^2 $$
\end{assumption}
\begin{assumption}
	\textbf{Bounded variance} $$\E  $$
\end{assumption}
Besides, to analysing a limitation of FL, other limitations usually are not counted. In \cite{}When the gradients of the loss function over our function approximator (NN) are unbiased, there are already convergence analyses given. In FL however, local gradients are computed over different data distributions and the updates aggregated from them are biased, which makes this analysis very hard. (To the best of our knowledge, nobody gave any meaningful results for this problem)
 
\section{Experimental results}
In most cases, the assumption of independent and identically distributed samples across local nodes does not hold for federated learning setups. Under this setting, the performances of the training process may vary significantly according to the unbalancedness of local data samples as well as the particular probability distribution of the training examples (i.e., features and labels) stored at the local nodes. To further investigate the effects of non-iid data, the following description considers the main categories presented in the by Peter Kiarouz and al. in 2019.

Federated learning parameters
Once the topology of the node network is chosen, one can control different parameters of the federated learning process (in opposition to the machine learning model's own hyperparameters) to optimize learning:

Number of federated learning rounds:T\\
Total number of nodes used in the process: K\\
Fraction of nodes used at each iteration for each node: C\\
Local batch size used at each learning iteration: B\\
Other model-dependent parameters can also be tinkered with, such as:\\
Number of iterations for local training before pooling: $N$
Local learning rate: $\eta $\\
The dataset we choose to examine is MNIST dataset. To simulate the non-IID data distribution
by setting each client to hold only digits from one certain
class and the number of clients from the same class to be
N=10. To simulate the data unbalance, we let the number of
samples on each client roughly follow a normal distribution
with mean  and variance .\\
\textbf{Logistic model and 2NN }
\begin{center}
	\includegraphics[scale=0.4]{2nn-log.png}	
	\figurename[2]{. Logistic model and 2 hidden layers NN model}
\end{center}
\textbf{Impact of parameters}
\begin{center}
		\includegraphics[scale=0.45]{lr.png}
	\figurename[3]{. Logistic model and 2 hidden layers NN model}
\end{center}
\textbf{Impact of balance}
\begin{thebibliography}{5}
	\bibitem{cite1}
	F. Sattler, S. Wiedemann, K. Müller, W. Samek,  (2019), \textit{Robust and communication-efficient federated learning from non-iid data},IEEE transactions on neural networks and learning systems,
	\bibitem{cite2}
	H. Brendan McMahan, Eider Moore, Daniel Ramage, (2017)  \textit{Communication-Efficient Learning of Deep Networks
		from Decentralized Data}, \href{https://arxiv.org/abs/1602.05629}{https://arxiv.org/abs/1602.05629}  
	\bibitem{cite3}
	\textit{TensorFlow Federated},\href{ https://github.com/tensorflow/federated }{https://github.com/tensorflow/federated}
	\bibitem{cite4}
	\textit{Advanced Machine Learning Systems Course} , (2017),  \href{https://www.cs.cornell.edu/courses/cs6787/2017fa/}{https://www.cs.cornell.edu/courses/cs6787/2017fa/}  	
	\bibitem{cite5}
	X. Li, K. Huang, W. Yang, S. Wang, Z. Zhang, (2019)  \textit{On the Convergence of FedAvg on Non-IID Data}, \href{https://arxiv.org/abs/1907.02189}{https://arxiv.org/abs/1907.02189}	
\end{thebibliography}

	\section*{Apendix}
\textbf{Which cases we get poor-quality results when performing the classical FedAvg?} \\
\begin{theorem}
	
\end{theorem}
\end{document}